统计学习导论-支持向量机
========================================================

# 最大边界分类器

## 超平面

- p维空间里(p-1)维子空间
- $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p = 0$ 定义一个p维超平面，X落在超平面上
- p维空间中点X不在超平面上就在其两侧

## 超平面分类

- n*p矩阵X分为两类Y-1或1
- 代入超平面大于0为1，小于0为-1，有$Y*\beta*X > 0$ 表示分类正确
- 构建训练函数$f(x^*) = \beta_0 + \beta_1 X_1^* + \beta_2 X_2^* + ... + \beta_p X_p^*$ 正数表示为1，负数为-1，距离0越远表示距离超平面越远，越近表示分类越不确定，判定边界为线性

## 最大边界分类器

- 最大边界超平面：距离边界最近的距离的所有超平面中距离边界点最远的那个超平面
- 分类良好但容易在p大时过拟合
- 形成最大边界分类器所需要的边界点为支持向量，用以支持最大边界超平面
- $f(x^*)*y_i$在系数平方和为1时为点到平面的垂直距离，最小化后最大化这个距离是求最大边界超平面的关键

# 支持向量分类器

- 有些情况不存在超平面，需要求一个软边界来适配最多的分类，这就是支持向量分类器
- 因为是软边界所以允许在超平面或边界一边出现误判
- 计算上还是为最小化最大化距离，但分类上距离要乘以$1 - \epsilon_i$项，也就是松弛变量
- 松弛变量大于0表示边界误判，大于1表示超平面误判，总和为C，表示边界的容忍度，越大分类越模糊
- C可通过交叉检验获得，控制bias-variance权衡
- 只有边界内观察点影响超平面的选择，这些点为支持向量，是形成模型的关键
- 与LDA不同，使用部分数据，与logistic回归类似

# 支持向量机

- 非线性条件下可以考虑将超平面理解为非线性超平面，提高样本维度换取分类效果
- 加入多项式等非线性描述后计算量不可控
- 支持向量机通过核来控制非线性边界
- 通过样本内积来解决支持向量分类问题
- 线性支持向量分类器$f(x) = \beta_0 + \sum_{i = 1}^{n} \alpha_i < x,x_i >$ 只有支持向量在解中非0，现在只需要支持向量的内积就可以求解
- 内积可以推广为核函数，核函数可以采用非线性模式
- $f(x) = \beta_0 + \sum_{i = 1}^{n} \alpha_i K( x,x_i )$ 径向基核函数较为常用
- 使用内积的核函数计算上简单且等价与高维空间超平面分类

## 多于二分类

- 一对一分类：对比$K \choose 2$个分类器在检验集中的效果，通过计数来选择分类结果
- 一对多分类：对比K个与剩下的K-1个分类，分类结果最远的认为属于那个分类

# 与logistic回归关系

- 中枢损失，对关键点敏感
- 传统方法也可以借鉴核函数观点视同
- 支持向量无法提供参数概率信息，采用核函数的logistic回归可以，计算量大
- 分类距离较远，支持向量机会比logistic回归好一点
- 支持向量机是计算机背景，logistic回归是概率背景