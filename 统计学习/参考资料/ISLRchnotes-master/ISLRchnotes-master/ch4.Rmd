统计学习导论-分类
========================================================

# logistic回归

- 因变量以概率形式出现
- $p(X) = \frac {e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}$
- 变形后$\frac {p(X)}{1 - p(X)}$ 为胜率，比概率应用更实际些，去对数后为对数胜率（logit）
- 因变量$p(X)$与自变量间关系非线性
- 用极大似然估计确定参数，似然函数为$l(\beta_0, \beta_1) = \prod_{i:y_i = 1} p(x_i)\prod_{i':y_{i'} = 0} (1 - p(x_{i'}))$，该函数取最大值
- 线性回归中，最小二乘法为极大似然估计的特例
- 混杂因素的解释上要考虑单因素回归与多元回归
- 多响应logistic回归一般被判别分析取代

# 线性判别分析

- 使用原因：分类离散时logistic回归不稳定，n小X正态时更稳定，适用于多响应
- 贝页斯理论：$Pr(Y = k|X = x) = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_lf_l(x)}$ 其中$\pi$ 代表先验概率，估计$f_k(X)$需要对$x$的分布作出假设
- 自变量为1时，假定$f_k(x)$分布为正态的，有$f_k(x) = \frac{1}{\sqrt{2 \pi} \sigma_k} exp(- \frac{1}{2 \sigma_k^2} (x - \mu_k)^2)$，代入可得$p_k(x)$，取对数有$\sigma_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(\pi_k)$，使$\sigma_k(x)$最大的分类方法为判定边界
- 贝页斯分类器需要知道所有分布参数，实际中会采用线性判别分析（LDA），通过以下训练集估计方法来插入贝页斯分类器：$\hat \pi_k = n_k/n$、$\hat \mu_k = \frac{1}{n_k} \sum_{i:y_i = k} x_i$ 与 $\hat \sigma^2 = \frac{1}{n - K} \sum_{k = 1}^K \sum_{i:y_i = k} (x_i - \hat \mu_k)^2$
- 线性体现在判别函数$\hat \sigma_k(x)$的形式是线性的
- 自变量多于1时，假设自变量均来自多元正态分布的分类
- 列连表，表示假阳性，假阴性，可计算灵敏度与特异性
- LDA是对贝页斯分类的模拟，旨在降低总错误率，因此灵敏度与特异性区分并不明显，可根据实际需要调节
- ROC曲线用来展示两种错误，横坐标假阳性，纵坐标真阳性

# 二次判别分析（QDA）及其它

- 不同于LDA，二次判别分析考虑各分类参数中方差不同而不是相同，引入了二次项
- 对分类描述更为精细，但容易过拟合，样本较少，LDA优先
- 对比logistic回归，两者数学形式相近，取值上logistic回归使用极大似然法，LDA使用共方差的高斯分布假设，结论多数条件一致，但随假设不同而不同
- KNN更适用于非线性关系，标准化很有必要，QDA相对温和