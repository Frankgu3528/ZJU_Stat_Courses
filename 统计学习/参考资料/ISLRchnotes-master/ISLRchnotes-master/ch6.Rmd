统计学习导论-线性模型选择与正则化
========================================================

- 最小二乘法（OLS）容易解释，预测性能好，但不万能
- 预测准确性上，当p>n时，模型方差变大
- 模型解释上，p过多需要去除，进行模型选择

# 模型选择方法

## 子集选择

- 从p个自变量中选出与模型响应相关的进行建模
- 使用devianc，最大化为最优子集
- 最佳子集选择：p个自变量$p \choose k$ ，计算$RSS$与$R^2$，$RSS$要小，$R^2$要大，选择最佳的
- 步进法：p值过大，计算负担重，采用逐步改进法进行模型选择
- 向前步进选择：从0个自变量开始加，第k个自变量选择p-k个模型，如果$RSS$与$R^2$表现好就保留，递近选择变量，不保证选择最佳模型，p值较大优先考虑
- 向后步进选择：从p个自变量开始减，如果第k个自变量在模型$RSS$与$R^2$中没表现，就剔除进行变量选择，不保证选择最佳模型，适用于p值较小的情况(较大可能无法拟合)
- 步进选择构建$1 + p(p+1)/2$个模型，最佳子集法需要构建$2^p$个模型
- 混合模型:向前选择，之后向后验证，剔除不再提高效果的模型

### 测试集误差估计

- $RSS$与$R^2$评价的是训练集拟合状况，不适用于估计测试集误差
- 估计测试集误差可以构建统计量调节训练集误差或直接通过验证集来估计
- Mallow’s $C_p$：$C_p = \frac{1}{n} (RSS + 2d\hat \sigma^2)$ $d$代表使用的变量数，$\hat \sigma^2$是对模型方差的估计
- AIC：$AIC = -2logL + 2 \cdot d$ 极大似然估计，线性模型下$C_p$与AIC实质等同
- BIC：$BIC = \frac{1}{n}(RSS + log(n)d\hat \sigma^2)$ n是样本数，大于7时BIC会比$C_p$选择更轻量的模型
- 调节$R^2$：$Adjusted$ $R^2 = 1 - \frac{RSS/n-d-1}{TSS/(n - 1)}$ 值越大，测试集误差越小
- 不同于$C_p$，AIC，BIC有严格的统计学意义，调节$R^2$虽然直观，但理论基础相对薄弱，单纯考虑了对无关变量的惩罚
- 验证与交叉验证：直接估计测试集误差而不用估计模型方差
- 单标准误原则：先计算不同规模测试集$MSE$的标准差，选择曲线中最小测试集误差一个标准误内最简单的模型

## 收缩

- 对系数估计进行收缩，接近0或等于0进行变量选择

### 岭回归

- 不同于最小二乘估计对$RSS$的最小化，岭回归最小化$RSS + \lambda \sum_{j = 1}^{p} \beta_j^2$，其中$\lambda$为调谐参数，后面一项为收缩惩罚，是个$l_2$范数，使参数估计逼近0，选择合适$\lambda$很重要，可用交叉检验来实现
- 因为范数大小影响模型惩罚项，所以进行岭回归前要做标准化处理$$\bar x_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n} \sum_{i = 1}^n (x_{ij} - \bar x_j)^2}}$$
- 岭回归的参数$\lambda$与范数收缩状况可看作最小$MSE$的函数来表现偏差-误差均衡
- 岭回归适用于最小二乘回归产生方差较大的情况，同时，计算负担较小，只伴随$\lambda$取值范围变化而变化

### Lasso

- 形式与岭回归一致，最小化$RSS + \lambda \sum_{j = 1}^{p} |\beta_j|$，使用$l_1$范数
- 岭回归参数同步收缩接近0，Lasso可以通过软边界直接收缩到0实现变量选择，产生稀疏模型，想像超球体与超多面体与超球面的接触
- 贝页斯视角下，岭回归与lasso关于线性模型系数的先验分布是不同的：前者为高斯分布，接近0时平坦，后验概率等同最优解；后者为拉普拉斯分布，接近0时尖锐，先验概率系数接近0，后验概率不一定为稀疏向量
- 岭回归与Lasso分别适用于真实模型自变量多或少的情况，并不广谱，考虑交叉检验来进行选择
- 交叉检验也可用来选择$\lambda$, 通过选择的自变量参与建模

## 降维

- 前提是自变量间不独立，将p个自变量向量投影到M维空间(M < p)，使用投影M拟合线性回归模型 $\sum_{m = 1}^{M}\theta_m z_{im} = \sum_{m = 1}^{M} \theta_m \sum_{j = 1}^{p} \phi_{jm}x_{ij} = \sum_{j = 1}^p \sum_{m = 1}^{M} \theta_m \phi_{jm} x_{ij} = \sum_{j = 1}^{p} \beta_j x_{ij}$
- 主成分：各自变量在主成分方向上方差最大
- 主成分回归(PCA)：实际为无监督算法，得到主成分后作为新变量进行最小二乘回归，认为因变量与自变量变异最大的方向一致，需要仔细检验这个假设，主成分个数的选择影响模型效果
- 岭回归疑似为主成分回归的连续版，两者都需要标准化，效果也相近
- 偏最小二乘(PLA)：第一个投影方向为因变量与自变量回归方向，后续投影是对残差投影方向的回归，重复得到监督学习的效果
- PLA通常并不比PCA更好，引入了监督算法提高了偏差

## 高维数据

- n远远少于p或接近的数据
- 最小二乘估计在n小于p时残差为0，太过精细
- $C_p$，AIC，BIC方法因为有参数$\hat \sigma^2$需要估计，而这个参数会在高维数据下变成0，调节$R^2$也会变成1
- 高维诅咒：正则化或收缩对高维方法产生影响，合适调谐参数十分重要，测试集误差必然增长
- 引入新变量会对预测产生不可知影响，选出的自变量并非不可替代，结果用独立验证集误差或交叉检验误差描述