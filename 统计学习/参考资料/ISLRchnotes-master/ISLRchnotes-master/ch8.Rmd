统计学习导论-决策树
========================================================

# 回归树

- 将因变量按自变量分区间，每个区间内预测值一致，直观易解释
- $\sum_{j = 1}^J\sum_{i \in R_J}(y_i - \hat y_{R_j})^2$
- 计算困难，使用自上而下的贪心算法
- 递归二元分割：构建树过程每个节点都选最佳分割点，也就是分割后残差最小的变量与数值
- 算法在叶样本数为5时结束
- 树修剪，选择训练集误差最小的子树，引入调谐因子$\alpha$
- 最小化$\sum_{m = 1}^{|T|} \sum_{i:x_i \in R_m} (y_i - \hat y_{R_m})^2 + \alpha|T|$ 类似lasso算法
- 确定\alpha要用交叉检验，之后选出特定模型

# 分类树

- 因变量为分类变量，RSS用分类错误率代替
- $E = 1 - max_k(\hat p_{mk})$但分类错误率对树生长并不敏感，应采用其他指标
- Gini系数：$G = \sum_{k = 1}^K\hat p_{mk}(1 - \hat p_{mk})$，分类越准，值越小，衡量端纯度
- cross-entropy：$D = - \sum_{k = 1}^K\hat  p_{mk} log\hat p_{mk}$，与Gini系数相似，描述一致
- 如果以修剪树为目标，指标应选择分类错误率
- 节点产生相同预测说明预测纯度不同，可靠性不同
- 与线性模型相比，适用数据种类不同，借助可视化判断
- 优点：容易解释，适用于决策，容易出图，处理分类问题简单
- 缺点：预测准确率低于其他常见回归与分类方法

# Bagging

- 决策树方法相比线性回归模型方差很大
- 引入Bootstrap，通过平均构建低方差模型
- $\hat f_{avg}(x) = \frac{1}{B} \sum_{b = 1}^B \hat f^{*b}(x)$ 
- 不修剪，通过平均降低方差
- 对于分类变量，通过投票，少数服从多数得到答案
- 误差估计通过包外样本（OOB）进行交叉检验并进行树的选择，降低计算成本
- 变量权重在Bagging中不易衡量，可通过衡量每棵树的RSS或者Gini系数在进行一次变量分割后RSS下降程度并进行排序取得
- 该方法可应用于其他统计模型

# 随机森林

- bagging中使用所有的变量进行选择，但是会更易出现共相关变量，方差降低不多
- 随机森林的核心在于强制使用较少的自变量，为其他自变量提供预测空间进而提高模型表现
- 变量数一般选择为$\sqrt p$
- 表现会比bagging好一些

# Boosting

- 通用的统计学习方法
- 树生长基于先前的树，不使用bootstrap，使用修改过的原始数据
- 先生成有d个节点的树，之后通过加入收缩的新树来拟合残差，收缩因子为$\lambda$，呈现层级模式，最后模型为$\hat f(x) = \sum_{b = 1}^B \lambda \hat f^b(x)$
- boosting学习缓慢，一般学习较慢的学习效果更好
- 三个参数：树个数B（交叉检验），收缩因子$\lambda$（控制学习速率），树节点数（一般为1，为交互作用深度，控制涉及变量）
- 深度d为1时是加性模型
- 随机森林与Boosting产生的模型都不好解释