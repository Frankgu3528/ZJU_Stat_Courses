# 基于LSTM的诗歌续写

### 训练

原文件里的学习率1e-5对于这个任务太小了，经过几百个epoch的训练也只能达到下面的样子：

```
远上寒山石径斜,。。。。，。。，。。，。，。。。。，。。。。。。，。。。。
远上寒山石径斜，不见，不不不不不。不有不不不不，不人不不人，不得不不不之。
```

调整学习率为1e-3，将`self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers)`中num_layers改成3后，学习效率有了明显改进，10个epoch就能生成一些完整的诗句了。不过较少的epoch训练出来的网络对于长的诗篇生成能力不强: 

```
远上寒山石敬谢，一声初断一枝花。春风吹雨春风起，万里春风一片春
一曲不知三十日，一年无事一时来。云间一曲春风起，月下春风一片春
```

在上面的结果中，前面4句有模有样，但第7,8句就简直在抄第3,4句（就像高中写作文的我一样）。

### 训练细节

**最终**我训练了500个epoch，在2080ti上100个epoch大概耗时50min。

并在第200个epoch的时候把学习率见到5e-4，第300个epoch的时候在减到1e-4,400的时候是减到1e-5.

实验的时候每个epoch都会输出一遍loss，在实验中我观察到每次loss在某个水平徘徊很久后，我手动减少lr的方法是有效的，可以使loss再下降一点。

（实际的loss下降图类似阶梯图形，每减少一点lr能使loss下一个台阶，很可惜训练的时候没有对loss画图，再训练一次耗时太久了就算了）

 (如果一直选用1e-3的lr，loss大概在2.2左右就稳定了，但如果像上面一样逐步减小lr，最后loss能下降到1左右)

### Usage

训练：

```
python train.py
```

生成：

```
python predict.py
```

### 测试

下面是对生成风格的一些测试，我们用毛主席的诗句“钟山风雨起苍黄”开头，我们认为给定几个十分有偏向性的perfix_words，看看模型会不会根据perfix_words做出一些风格上的调整，结果如下：

<img src="https://frank-first.oss-cn-hangzhou.aliyuncs.com/images/image-20230603211737161.png" alt="image-20230603211737161" style="zoom:50%;" />

* 前三个perfix_words都是豪放型的，可以看到生成的诗句中也带有了“战马”，“肃鼓”，“蓬门”等比较豪放风格的意象；
* 后两首给的perfix_words则描绘的是江南水乡的场景，生成的诗句中也带有了“舟中照客愁", ”江南“，”扬州“，”春潭”等风格的意象。

### 一些问题

* 生成的诗句长度不固定，大部分时候能做到7个字一个标点，但有时也出现每句字数不对的情况：

  ```
  start_words= 钟山风雨起苍黄  perfix_words= 大漠金戈孤雁
  钟山风雨起苍黄，古萝一树濛。掩。
  忽阑倚银台，含光影入帘门。风飘。
  ，绣初锦，禁阳天。春风韶意，不。
  相见。郎君不见此心期，昨夜梦魂。
  ```

  猜测作为原始数据的`tang.npz` 中包含的诗句有五言诗，七言诗甚至其他格式的比较混乱，所以生产诗句时网络有时候在错误的地方输出。如果原数据只要七言诗应该会好很多。

* 中国古代的诗歌有押韵的要求，但目前这个网络生成的诗歌还几乎没有押韵的样子。可能迭代更多次数能慢慢出现简单的押韵吧。

